{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The following lines enable automatic reloading of modules in an IPython/Jupyter environment.\n",
                "# They work exactly like the commented lines below, but avoid errors when not running in such an environment.\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n",
                "\n",
                "try:\n",
                "    # Only defined inside IPython/Jupyter\n",
                "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
                "except (NameError, AttributeError):\n",
                "    # Not running in IPython â†’ just ignore\n",
                "    pass\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initializing answer variable\n",
                "answer = {}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Some libs that we will use\n",
                "import torch\n",
                "import random\n",
                "import numpy as np\n",
                "import json_tricks\n",
                "import lovely_tensors as lt\n",
                "\n",
                "# Making tensor printouts better\n",
                "lt.monkey_patch()\n",
                "\n",
                "# Adding sources to the pythonpath\n",
                "import sys\n",
                "root_path = '../../../..'\n",
                "sys.path.append(root_path)\n",
                "\n",
                "import dotenv\n",
                "dotenv.load_dotenv(dotenv.find_dotenv(root_path + '/.env'))\n",
                "\n",
                "# Importing sources of our project\n",
                "import src\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 0: Prepare the environment"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For that you have to fill out the function in `src/utils/seed.py`\n",
                "\n",
                "Inside this function you should initialize the libs:\n",
                "- `numpy`\n",
                "- `random`\n",
                "- `torch`\n",
                "\n",
                "It is important for the reproducibility of your code and is always important when running experiments\n",
                "\n",
                "Note that in `torch` you have to seed both `cpu` part of the package along with `gpu` part of the package\n",
                "\n",
                "Yet another important thing to do there is to switch `gpu` to deterministic backend. Otherwise the experiments will not be fully reproducible"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "src.utils.seed.seed_all(0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training a Fully Connected Feedforward Neural Network"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this exercise we will train an FCNN to classify digits into 10 classes (0, 1, 2, ..., 9).\n",
                "\n",
                "To build such network, we will use:\n",
                "- Fully Connected NN\n",
                "- Adam Optimizer\n",
                "- MNIST dataset\n",
                "- Accuracy metrics\n",
                "- Softmax final activation loss function (but we will embed it into the loss function)\n",
                "- Cross-Entropy loss\n",
                "\n",
                "But what really is important in this exercise is that we will build all the elements that we will in the future use to train all other networks. Thus, this exercise is needed to settle the basis for all our future experiments. If you will work with training neural networks, you will use all that we will touch in this notebook in different scenarios.\n",
                "\n",
                "Thus, this notebook is way closer to the real training scenarios than the previous ones."
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1: Prepare the data\n",
                "\n",
                "What you will be doing in this task is preparing your first dataloader class for one of the most popular datasets for Machine Learning -- MNIST\n",
                "\n",
                "The task is to fill out `src/datasets/mnist_simple.py` file so that you get a correctly working class of dataset (this file should open in parallel to your notebook).\n",
                "\n",
                "You have to fill out the following parts of code:\n",
                "- `__init__` function (a constructor)\n",
                "- `__len__` function (an operator of `len(object)`)\n",
                "- `__getitem__` function (that enables indexing in form of `object[index]`)\n",
                "\n",
                "You will be doing all that when you will be writing your own datasets with one exception: your classes will have significantly more complicated logics.\n",
                "\n",
                "The task here is:\n",
                "- In `__init__` get MNIST dataset from `torchvision.datasets` and initialize it. \n",
                "    Store the data in your home directory (`~/`) to avoid overloading your repo. \n",
                "    Note that you have to account for training and validation datasets that are controlled by the flag `train`\n",
                "- In `__getlen__` the task is to return the number of samples in your data. \n",
                "    It should be number of images in the dataset object\n",
                "\n",
                "- Lastly, you have to implement indexing operator `__getitem__(self, index)`\n",
                "    In this method you should:\n",
                "    - get the image\n",
                "    - get the label\n",
                "    - preprocess the image. This step contains:\n",
                "        - transform the image to a float tensor (the reason is that half is not suppoerted for CPUs)\n",
                "        - a good practice is to normalize the input data to have mean 0 and variance 1, so we should normalize it.\n",
                "            Althouth we do not know exactly the mean and the variance of the dataset, but assuming that every pixel\n",
                "            is in the range 0-255, we can assume that the mean is 127 and the variance is 127.\n",
                "            So we can approximately normalize the input by (x / 255) * 2 - 1\n",
                "            Yes, that is not an exact normalization, but haaving data within range (-1, 1) is a good approximation of normalization\n",
                "    - return the image and the label\n",
                "\n",
                "After that fill out the cell below that initializes objects of the class"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MNIST_train = ...\n",
                "MNIST_valid = ...\n",
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check that the data is prepared:"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_sample = MNIST_train[0]\n",
                "valid_sample = MNIST_valid[0]\n",
                "\n",
                "X_train = train_sample['image']\n",
                "X_valid = valid_sample['image']\n",
                "y_train = train_sample['label']\n",
                "y_valid = valid_sample['label']\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## This checks are for dataset verification\n",
                "answer['X_train.dtype'] = str(X_train.dtype)\n",
                "answer['y_train.dtype'] = str(y_train.dtype)\n",
                "answer['X_valid.dtype'] = str(X_valid.dtype)\n",
                "answer['y_valid.dtype'] = str(y_valid.dtype)\n",
                "answer['X_train.shape'] = X_train.shape\n",
                "answer['X_valid.shape'] = X_valid.shape\n",
                "answer['y_train.shape'] = y_train.shape\n",
                "answer['y_valid.shape'] = y_valid.shape\n",
                "answer['X_train.mean'] = float(X_train.mean())\n",
                "answer['y_train.mean'] = float(y_train.float().mean())\n",
                "answer['X_valid.mean'] = float(X_valid.mean())\n",
                "answer['y_valid.mean'] = float(y_valid.float().mean())\n",
                "\n",
                "print(X_train.dtype, X_valid.dtype, y_train.dtype, y_valid.dtype)\n",
                "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
                "\n",
                "print(X_train)\n",
                "print(X_valid)\n",
                "print(y_train)\n",
                "print(y_valid)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Neural networks are usually trained in float values. Usually in `float32` or `float16`. In this exercise we will use `float32` precision.\n",
                "\n",
                "Convert your data to `float` type"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us visualize the data that we have"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "plt.imshow(X_train)\n",
                "plt.show()\n",
                "print(y_train)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2: Build the code for the network"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is time to create our neural network (edit the file `src/models/feedforward/simple_fcnn.py`, should be open automatically with this chapter).\n",
                "\n",
                "In pytorch to enable all the magic for training, people enherit their models from `torch.nn.Module`.\n",
                "\n",
                "This enables many utilities that will be used in the future.\n",
                "\n",
                "The main two methods that we have to fill are:\n",
                "- `__init__` method that creates all modules of Neural Network\n",
                "    NOTE: you should also intialize the parent class's instance(`torch.nn.Module` by calling `super().__init__()`)\n",
                "- `__call__` method that is used to calculate preditions\n",
                "\n",
                "\n",
                "We will create a network for quite a general case of Fully-Connected Neural Network\n",
                "\n",
                "Here is the network that you should create:\n",
                "- it should have the structure:\n",
                "    - `network`:\n",
                "        - `Linear channels[0] -> channels[1] -> activation`\n",
                "        - `Linear channels[1] -> channels[2] -> activation`\n",
                "        - and so on\n",
                "    - `classifier`:\n",
                "        - `Linear channels[n] -> n_classes`\n",
                "- the signals that you create should traverse the initialized modules according to the order above\n",
                "\n",
                "That is the simple n-layer fully connected network.\n",
                "\n",
                "The network should have two parts:\n",
                "- `backbone` (containing all the layers except for the last linear layer)\n",
                "- `classifier` (containing only the last layer)\n",
                "\n",
                "We like to isolate the classifier separately because it is a very special layer of the network.\n",
                "\n",
                "It also is a very good practice to wrap the modules where one block is executed after another\n",
                "into a `torch.nn.Sequential`.\n",
                "\n",
                "Note that in theory, we should use a SoftMax activation in the end. \n",
                "But in our case, this activation will be joined with the loss function due to mathematical reasons."
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us check that your network actually works"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "simple_network = src.models.feedforward.simple_fcnn.SimpleFCNN([28 * 28], n_classes=10)\n",
                "\n",
                "src.utils.deterministic_init(simple_network)\n",
                "\n",
                "check_input = {'image': torch.randn(10, 28 * 28)}\n",
                "check_output = simple_network(check_input['image'])\n",
                "\n",
                "answer['check_result'] = src.utils.detach_copy(check_output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 3: Create loss function and optimizer\n",
                "\n"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "All right, so now we have prepared the dataset and the network.\n",
                "We are still missing several other modules that are important for model training:\n",
                "- optimizer\n",
                "- loss function\n",
                "- metrics\n",
                "\n",
                "To train this network we will use the following components:\n",
                "- `AdamW` as an optimizer (`torch.optim.AdamW`) with standard parameters and `lr` equal to $3 \\cdot 10^{-4}$\n",
                "- `CE` as a loss function (`torch.nn.CrossEntropy`). Note that this loss function already includes `SoftMax` activation function (and we should remember that this activation is compatible with this loss function). We could code it ourselves, it is not hard, but the standard implementation from pytorch is numerically more stable\n",
                "- `accuracy` as a metrics (we will use the one coded in `torchmetrics`)\n",
                "\n",
                "There are also several training tricks that are very nice to use.\n",
                "As such, we will use the learning rate scheduler `torch.optim.schedulers.ReduceLROnPlateu`\n",
                "\n",
                "Let us prepare what we need now!"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torchmetrics\n",
                "\n",
                "loss = ...\n",
                "optimizer = ...\n",
                "scheduler = ...\n",
                "metrics = ...\n",
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "answer['loss'] = str(loss)\n",
                "answer['optimizer'] = str(optimizer)\n",
                "answer['scheduler'] = str(scheduler)\n",
                "answer['metrics'] = str(metrics)\n",
                "\n",
                "print(loss)\n",
                "print(optimizer)\n",
                "print(scheduler)\n",
                "print(metrics)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 4: Create DataLoaders"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now it is time to start the training process\n",
                "\n",
                "Firstly, we need to turn our datasets into dataloaders (dataloaders know, how to create batches of data and takes care of shuffling them)\n",
                "\n",
                "You should use the class `torch.utils.data.DataLoader` to create training and validation dataloaders.\n",
                "\n",
                "What is important here is that you should shuffle the data for training, while you should not do it during validation.\n",
                "\n",
                "Also the last batch of the training dataloader may be deficient and should be dropped (there is a special option in the DataLoader class dedicated for that).\n",
                "\n",
                "Use batch size that is given below"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import trange, tqdm\n",
                "import neptune\n",
                "import os\n",
                "\n",
                "batch_size = 32\n",
                "\n",
                "train_dl = torch.utils.data.DataLoader(\n",
                "    MNIST_train,\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    drop_last=True)\n",
                "\n",
                "valid_dl = torch.utils.data.DataLoader(\n",
                "    MNIST_valid,\n",
                "    batch_size=batch_size,\n",
                "    shuffle=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 5: Create training loop"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Firstly, we will use a service that allows to share the training reports. I strongly recommend using mlflow for your experiments, but for the sake of being able to share the results, we will use [neptune.ai](https://neptune.ai/)\n",
                "\n",
                "Create an account there, create your first experiment (and name it something like `MNIST FCNN`)\n",
                "\n",
                "Then create run with `neptune.ai` in your code.\n",
                "\n",
                "After that you will be able to see on that website the flow of your experiment.\n",
                "\n",
                "Then it is time to create the training cycle.\n",
                "\n",
                "Training cycles usually are custom for every network, because of that we will not create one for all the cases.\n",
                "\n",
                "Training cycles run in epochs each containing 2 stages:\n",
                "- training\n",
                "- validation\n",
                "\n",
                "Here is what you should do in the training stage:\n",
                "1. extract training batch from training dataloader\n",
                "2. switch the network to training state (`model.train()`)(will be important for batchnorms and some other modules)\n",
                "2. generate predictions from the training inputs using the model (`model(inputs)`)\n",
                "3. calculate the value of the loss that compares the predictions to the targets\n",
                "4. perform backpropagation (`loss_val.backward()`)\n",
                "5. make optimization step  with optimizer (`optimizer.step()`)\n",
                "6. reset optimizer's gradients (`optimizer.zero_grad()`)\n",
                "7. switch the network to validation state (`model.eval()`)\n",
                "8. evaluate training the accuracy of your model and update metrics that keep track of accuracy and loss (averaged throughout the training step)\n",
                "\n",
                "This should be done in iterations for all training batches that come out from `train_dl`\n",
                "\n",
                "Note that it is extremely important to do `detach` when you memorize accuracy and loss values so that after each iteration RAM is freed automatically by pytorch (because in case we plan to perform backprop, all the intermediate values are needed, and cannot be freed).\n",
                "\n",
                "Once in a while, if you experience RAM leakage, you should make sure that you detach metrics and loss values that you memorize. In case that does not help, delete all the variables manually and after that call pythons's garbage collector (`gc.collect()`). This will help pytorch with cleaning RAM.\n",
                "\n",
                "If you do not want to think about detaching and other aspects of graph backpropagation in some part of your code, you may use `torch.no_grad()` context. Exactly that statement we will use during validation stage of the cycle. This context also can be used as a decorator of a function that does not require gradient propagation.\n",
                "\n",
                "Now it is time to do validation:\n",
                "- make prediction for validation data\n",
                "- calculate accuracy for the predictions for both sets\n",
                "- calculate loss function for the predictions for both sets\n",
                "\n",
                "Note that the code should look very similarly to training cycle. The only difference is that this time we do not perform backpropagation and optimizer step.\n",
                "\n",
                "By the end of each epoch, submit the report about accuracies that you gotten and loss values to neptune.ai (run['losses/loss_value/train'].log(train_loss_value))"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_epochs = 5\n",
                "\n",
                "def train_model(model, n_epochs, train_dl, valid_dl, loss, optimizer):\n",
                "    train_loss_history = []\n",
                "    train_acc_history = []\n",
                "    valid_loss_history = []\n",
                "    valid_acc_history = []\n",
                "\n",
                "    ## INITILIZING EXPERIMENT TRACKER (neptune.ai, mlflow, tensorboard, etc)\n",
                "    # run = neptune.init_run(\n",
                "        ## YOUR CODE HERE\n",
                "    # )\n",
                "\n",
                "    for epoch in range(n_epochs):\n",
                "        train_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "        train_acc = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "        valid_loss = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "        valid_acc = {'enumerator': 0.0, 'denominator': 1.0e-8}\n",
                "\n",
                "        print(epoch + 1, '/', n_epochs)\n",
                "\n",
                "        for batch in tqdm(train_dl):\n",
                "            ...\n",
                "            ## YOUR TRAINING CODE HERE\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for valid_batch in tqdm(valid_dl):\n",
                "                ...\n",
                "                ## YOUR EVALUATION CODE HERE\n",
                "\n",
                "            print(valid_acc['enumerator'], valid_acc['denominator'], '<- Validation accuracy debug')\n",
                "\n",
                "            finalized_train_loss = train_loss['enumerator'] / train_loss['denominator']\n",
                "            finalized_train_accuracy = train_acc['enumerator'] / train_acc['denominator']\n",
                "\n",
                "            finalized_valid_loss = valid_loss['enumerator'] / valid_loss['denominator']\n",
                "            finalized_valid_accuracy = valid_acc['enumerator'] / valid_acc['denominator']\n",
                "\n",
                "            # Logging the progress to tracker\n",
                "\n",
                "            train_loss_history.append(finalized_train_loss)\n",
                "            train_acc_history.append(finalized_train_accuracy)\n",
                "            valid_loss_history.append(finalized_valid_loss)\n",
                "            valid_acc_history.append(finalized_valid_accuracy)\n",
                "\n",
                "    # Stopping the experiment tracker\n",
                "    # run.stop()\n",
                "    return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history\n",
                "\n",
                "train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_model(simple_network, n_epochs, train_dl, valid_dl, loss, optimizer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "answer['train_loss_history'] = train_loss_history\n",
                "answer['valid_loss_history'] = valid_loss_history\n",
                "answer['train_acc_history'] = train_acc_history\n",
                "answer['valid_ann_history'] = valid_acc_history\n",
                "\n",
                "json_tricks.dump(answer, '.answer.json')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 7. Experiment time!"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is the time for fun now!\n",
                "\n",
                "We have created everything that is needed for the network to train.\n",
                "\n",
                "Let us train some networks and visualize, what they see.\n",
                "\n",
                "With the code that you have created, make:\n",
                "- 1-layer FCNN (yes, simple Multiclass Logistic Regression)\n",
                "- 2-layer FCNN\n",
                "- 3-layer FCNN \n",
                "\n",
                "Train every of these networks for 100 epochs\n",
                "\n",
                "Check, what are the patterns that they learn\n",
                "\n",
                "To check the patterns, you have to take the weights of the first layer of the network, reshape them and plot."
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "one_layer_fcnn = src.models.feedforward.simple_fcnn.SimpleFCNN(\n",
                "    [28 * 28],\n",
                "    n_classes=10,\n",
                "    activation=torch.nn.LeakyReLU)\n",
                "\n",
                "## YOUR CODE HERE\n",
                "\n",
                "one_layer_optimizer = torch.optim.AdamW(one_layer_fcnn.parameters(), lr=1.0e-3)\n",
                "train_model(one_layer_fcnn, n_epochs, train_dl, valid_dl, loss, one_layer_optimizer)\n",
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "W = one_layer_fcnn.classifier.weight\n",
                "W = W.reshape(10, 28, 28)\n",
                "plt.imshow(W[0].detach())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## YOUR CODE HERE\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Time to think\n",
                "\n",
                "What converges faster\n",
                "- 1-layer NN (Logistic Regression)\n",
                "- 2-layer NN\n",
                "- 3-layer NN\n",
                "\n",
                "Maybe you want to train a deeper network? Say, with 10 layers?\n",
                "\n",
                "What do the patterns from 1-layer NN resemble? Should they?\n",
                "What do the patterns from 2-layer and 3-layer NN resemble? Should they?"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 8. More experiments!\n",
                "\n",
                "Try to train the networks created above with different actiavtion functions (Concentrate on 2-Layer NN):\n",
                "- Sigmoid\n",
                "- Tanh\n",
                "- ReLU\n",
                "- LeakyReLU\n",
                "- Swish\n",
                "\n",
                "Try to train them with optimizers\n",
                "- AdamW\n",
                "- SGD\n",
                "\n",
                "Which one is easier to tune?\n",
                "\n",
                "Try to train these networks on Google Colab for longer\n",
                "Find the best possible batch size (note that the batch sizes usually are selected as powers of 2)\n",
                "- 2\n",
                "- 4\n",
                "- 8\n",
                "- 16\n",
                "- etc.\n",
                "\n",
                "Share your best recepie with everyone and discuss, how did you improve your results"
            ],
            "outputs": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Lesson 5 Digits Recognition Video.ipynb",
            "provenance": [],
            "version": "0.3.2"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}